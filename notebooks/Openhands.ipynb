{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to infer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import cv2\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logger import Config, upload_to_kaggle\n",
    "\n",
    "from params import *\n",
    "from data.dataset import SignDataset\n",
    "from data.preparation import *\n",
    "\n",
    "from utils.metrics import *\n",
    "from utils.plots import plot_sample\n",
    "from utils.plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openhands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install omegaconf torchmetrics pytorch_lightning pytorchvideo hydra-core natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SLGCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(DATA_PATH, processed_folder=\"../input/openhands/\")\n",
    "\n",
    "df = df[df['sign'] == \"shower\"]\n",
    "\n",
    "dataset = SignDataset(df, max_len=30, train=False)\n",
    "\n",
    "BS = 256\n",
    "\n",
    "x = {}\n",
    "# batch = [dataset[idx] for idx in range(BS)]  # \n",
    "batch = [dataset[idx] for idx in np.random.randint(len(dataset), size=BS)]\n",
    "for k in batch[0]:\n",
    "    x[k] = torch.cat([d[k].unsqueeze(0) for d in batch])  # .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, out_torch = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = y.detach().numpy().argmax(-1)\n",
    "gt = x['target'].numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_classes = np.array(classes)[gt.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsasl_classes = pd.read_json('../input/wlasl/WLASL_v0.3.json')['gloss'].values\n",
    "pred_classes = wsasl_classes[pred.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred_classes == gt_classes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.where(pred_classes == gt_classes)[0]:\n",
    "    d = out_torch[idx]\n",
    "    data = {\n",
    "        \"x\": d[0],\n",
    "        \"y\": d[1],\n",
    "        \"type\": torch.tensor([1] * 7 + [2] * 10 + [3] * 10).unsqueeze(0).repeat(d.size(1), 1)\n",
    "    }\n",
    "\n",
    "    plot_sample(data, n_frames=9, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = out_torch[0]\n",
    "data = {\n",
    "    \"x\": d[0],\n",
    "    \"y\": d[1],\n",
    "    \"type\": torch.tensor([1] * 7 + [2] * 10 + [3] * 10).unsqueeze(0).repeat(d.size(1), 1)\n",
    "}\n",
    "\n",
    "plot_sample(data, n_frames=9, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenHandModel(omegaconf.OmegaConf.load(\"../input/weights/st_gcn/config.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_from_checkpoint_if_available(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, truths = model.test_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(preds.argmax(-1) == truths).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = model.datamodule.test_dataloader()\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 15\n",
    "out_torch = batch['frames'][idx]\n",
    "\n",
    "gt = wsasl_classes[batch['labels'][idx]]\n",
    "print(gt, gt in classes)\n",
    "\n",
    "data = {\n",
    "    \"x\": out_torch[0],\n",
    "    \"y\": out_torch[1],\n",
    "    \"type\": torch.tensor([1] * 7 + [2] * 10 + [3] * 10).unsqueeze(0).repeat(out_torch.size(1), 1)\n",
    "}\n",
    "\n",
    "plot_sample(data, n_frames=9, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_FOLDER = \"../logs/2023-03-16/42/\"\n",
    "EXP_FOLDER = \"../logs/2023-03-25/6/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(DATA_PATH, config.processed_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"fold\" not in df.columns:\n",
    "    folds = pd.read_csv(config.folds_file)\n",
    "    df = df.merge(folds, how=\"left\", on=[\"participant_id\", \"sequence_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_oof = np.load(EXP_FOLDER + \"pred_oof.npy\")\n",
    "score = accuracy(df['target'], pred_oof)\n",
    "print(f\"-> CV acc : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = pred_oof.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error'] = (df['target'] != df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('participant_id').agg(['mean', 'count', 'sum'])[['error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.sort_values(('error',  'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('sign').agg('mean')[['error']].sort_values('error', ascending=False).T\n",
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = json.load(open(DATA_PATH + \"sign_to_prediction_index_map.json\", \"r\"))\n",
    "classes = list(classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df['target'], df['pred'], normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['target'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "65000 / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = \"../input/glove.6B.50d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, \"rb\") as f:\n",
    "    for line in tqdm(f):\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word.decode(\"utf-8\")] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "rep = {\n",
    "    \"callonphone\": 'phonecall',\n",
    "    \"frenchfries\": 'fries',\n",
    "    \"glasswindow\": 'window',\n",
    "    \"hesheit\": 'he',\n",
    "    \"minemy\": 'my',\n",
    "    \"weus\": 'we',\n",
    "    \"haveto\": \"have\",\n",
    "    \"owie\": \"bruise\",\n",
    "}\n",
    "\n",
    "embed = []\n",
    "for c in tqdm(classes):\n",
    "    c = rep.get(c, c)\n",
    "#     vec = nlp(c).vector\n",
    "    vec = embeddings_index[c.lower()]\n",
    "    embed.append(vec)\n",
    "embed = np.array(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../output/embed.npy', embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.sqrt((embed[:, None] - embed[None]) ** 2).mean(-1)\n",
    "\n",
    "# dists = (embed[:, None] * embed[None]).sum(-1) / ((embed[None] ** 2).sum(-1) * (embed[:, None] ** 2).sum(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(classes))):  # gt\n",
    "    order = np.argsort(dists[i])\n",
    "    print(np.array(classes)[order[:5]])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(classes))):  # gt\n",
    "    for j in range(len(classes)):\n",
    "        n = cm[i, j]\n",
    "        if n > 50 and i != j:\n",
    "            s = f\"{classes[i]} predicted as {classes[j]} :\".ljust(32)\n",
    "            print(f\"{s} {n} / {cm[i].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    y_pred,\n",
    "    y_true,\n",
    "    cm=None,\n",
    "    normalize=\"true\",\n",
    "    display_labels=None,\n",
    "    cmap=\"viridis\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes and plots a confusion matrix.\n",
    "    Args:\n",
    "        y_pred (numpy array): Predictions.\n",
    "        y_true (numpy array): Truths.\n",
    "        normalize (bool or None, optional): Whether to normalize the matrix. Defaults to None.\n",
    "        display_labels (list of strings or None, optional): Axis labels. Defaults to None.\n",
    "        cmap (str, optional): Colormap name. Defaults to \"viridis\".\n",
    "    \"\"\"\n",
    "    if cm is None:\n",
    "        cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "#     cm = cm[::-1, :]\n",
    "\n",
    "    # Display colormap\n",
    "    n_classes = cm.shape[0]\n",
    "    im_ = plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "\n",
    "    # Display values\n",
    "    cmap_min, cmap_max = im_.cmap(0), im_.cmap(256)\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "    for i in tqdm(range(n_classes)):\n",
    "        for j in range(n_classes):\n",
    "            if cm[i, j] > 0.1:\n",
    "                color = cmap_max if cm[i, j] < thresh else cmap_min\n",
    "                text = f\"{cm[i, j]:.0f}\" if normalize is None else f\"{cm[i, j]:.1f}\"\n",
    "                plt.text(j, i, text, ha=\"center\", va=\"center\", color=color)\n",
    "\n",
    "    # Display legend\n",
    "    plt.xlim(-0.5, n_classes - 0.5)\n",
    "    plt.ylim(-0.5, n_classes - 0.5)\n",
    "    if display_labels is not None:\n",
    "        plt.xticks(np.arange(n_classes), display_labels)\n",
    "        plt.yticks(np.arange(n_classes), display_labels)\n",
    "\n",
    "    plt.ylabel(\"True label\", fontsize=12)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "plot_confusion_matrix(df['pred'], df['target'], display_labels=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
