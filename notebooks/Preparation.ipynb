{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "!pip install -q ../output/nvidia_dali_nightly_cuda110-1.23.0.dev20230203-7187866-py3-none-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# YoloX\n",
    "!pip install --no-index -qqq ../output/rsna-yolox/loguru-0.6.0-py3-none-any.whl\n",
    "!pip install --no-index -qqq ../output/rsna-yolox/thop-0.1.1.post2209072238-py3-none-any.whl\n",
    "!pip install --no-index -qqq ../output/rsna-yolox/protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n",
    "!pip install --no-index -qqq ../output/rsna-yolox/onnx-1.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install --no-index -qqq ../output/rsna-yolox/onnx_simplifier-0.4.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install --no-index -qqq ../output/rsna-yolox/Cython-0.29.33-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\n",
    "\n",
    "!pip install -qqq ../output/rsna-yolox/pycocotools-2.0.6-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "!mkdir -p /tmp/pip/cache/\n",
    "\n",
    "!cp ../output/rsna-yolox/protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/onnx-1.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/onnx_simplifier-0.4.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/onnx_simplifier-0.4.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/tensorboard_plugin_wit-1.8.1-py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/tensorboard-2.11.2-py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/Werkzeug-2.2.2-py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/google_auth-2.16.0-py2.py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/absl_py-1.4.0-py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/rsa-4.9-py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/pyasn1_modules-0.2.8-py2.py3-none-any.whl /tmp/pip/cache/\n",
    "!cp ../output/rsna-yolox/grpcio-1.51.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl /tmp/pip/cache/\n",
    "\n",
    "!pip install -qqq ../output/rsna-yolox/tensorboard-2.11.2-py3-none-any.whl --find-links /tmp/pip/cache/\n",
    "\n",
    "!rm -rf /tmp/pip/cache/YoloX\n",
    "!cp -r ../output/rsna-yolox/YOLOX /tmp/pip/cache/\n",
    "!cd /tmp/pip/cache/YOLOX; pip install -qqq --find-links /tmp/pip/cache/ -v -e . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Add UINT16 on Pytorch for feed_ndarray with DALI\n",
    "with open(\"/usr/local/lib/python3.8/dist-packages/nvidia/dali/plugin/pytorch.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "content_uint16 = content.replace(\n",
    "    \"    types.DALIDataType.INT8:    torch.int8,\",\n",
    "    \"    types.DALIDataType.INT8:    torch.int8,\\n    types.DALIDataType.UINT16:  torch.int16,\"\n",
    ")\n",
    "\n",
    "with open(\"/usr/local/lib/python3.8/dist-packages/nvidia/dali/plugin/pytorch.py\", \"w\") as f:\n",
    "    f.write(content_uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pydicom\n",
    "import dicomsdl\n",
    "import numpy as np\n",
    "import nvidia.dali.types as types\n",
    "from pydicom.filebase import DicomBytesIO\n",
    "from nvidia.dali.experimental import eager\n",
    "from nvidia.dali.types import DALIDataType\n",
    "from nvidia.dali.plugin.pytorch import feed_ndarray\n",
    "\n",
    "\n",
    "def dicomsdl_to_numpy_image(dicom, index=0):\n",
    "    info = dicom.getPixelDataInfo()\n",
    "    dtype = info[\"dtype\"]\n",
    "    if info[\"SamplesPerPixel\"] != 1:\n",
    "        raise RuntimeError(\"SamplesPerPixel != 1\")\n",
    "    else:\n",
    "        shape = [info[\"Rows\"], info[\"Cols\"]]\n",
    "    outarr = np.empty(shape, dtype=dtype)\n",
    "    dicom.copyFrameData(index, outarr)\n",
    "    return outarr\n",
    "\n",
    "\n",
    "def load_img_dicomsdl(f):\n",
    "    return dicomsdl_to_numpy_image(dicomsdl.open(f))\n",
    "\n",
    "\n",
    "def read_normalize_basic(dcm_path):\n",
    "    dicom = pydicom.dcmread(dcm_path, stop_before_pixels=True)\n",
    "\n",
    "    if dicom.file_meta.TransferSyntaxUID == \"1.2.840.10008.1.2.4.90\":\n",
    "        to_find = b\"\\x00\\x00\\x00\\x0C\"\n",
    "    elif dicom.file_meta.TransferSyntaxUID == '1.2.840.10008.1.2.4.70':\n",
    "        to_find = b\"\\xff\\xd8\\xff\\xe0\"\n",
    "    else:\n",
    "        to_find = None\n",
    "\n",
    "    if to_find is not None:\n",
    "        with open(dcm_path, \"rb\") as fp:\n",
    "            raw = DicomBytesIO(fp.read())\n",
    "            ds = pydicom.dcmread(raw)\n",
    "        offset = ds.PixelData.find(to_find)\n",
    "        bitstream = np.array(bytearray(ds.PixelData[offset:]), dtype=np.uint8)\n",
    "        # Decode image to GPU\n",
    "        img = eager.experimental.decoders.image(\n",
    "            [bitstream],\n",
    "            device=\"gpu\",\n",
    "            output_type=types.ANY_DATA,\n",
    "            dtype=DALIDataType.UINT16,\n",
    "        )[0]\n",
    "        # Wrap to Pytorch\n",
    "        img_torch = torch.empty(img.shape(), dtype=torch.int16, device=\"cuda\")\n",
    "        feed_ndarray(img, img_torch, cuda_stream=torch.cuda.current_stream(device=0))\n",
    "        img = img_torch.float().squeeze()\n",
    "\n",
    "    else:\n",
    "        # read on cpu then load\n",
    "        try:\n",
    "            img = load_img_dicomsdl(dcm_path)\n",
    "        except NotImplementedError:\n",
    "            img = pydicom.dcmread(dcm_path, stop_before_pixels=False).pixel_array\n",
    "        img = torch.from_numpy(img.astype(np.float32))\n",
    "        img = img.to(\"cuda\")\n",
    "\n",
    "    # Normalize image to 0 - 1\n",
    "    min_, max_ = img.min(), img.max()\n",
    "    img = (img - min_) / (max_ - min_ + 1e-6)\n",
    "\n",
    "    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        img = 1 - img\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoloX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/pip/cache/YOLOX/exps/example/custom/nano.py\n",
    "\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from yolox.exp import Exp as MyExp\n",
    "\n",
    "\n",
    "class Exp(MyExp):\n",
    "    def __init__(self):\n",
    "        super(Exp, self).__init__()\n",
    "        self.depth = 0.33\n",
    "        self.width = 0.25\n",
    "        \n",
    "        self.mosaic_prob = 0.0\n",
    "        self.mixup_prob = 0.0\n",
    "        self.hsv_prob = 0.0\n",
    "        self.degrees = 2.0\n",
    "        self.translate = 0.05\n",
    "        self.shear = 0.0\n",
    "        self.multiscale_range = 3 # 5\n",
    "        self.random_size = (20, 34)\n",
    "        self.mosaic_scale = (0.5, 1.5)        \n",
    "        self.input_size = (1024, 1024)\n",
    "        self.test_size = (1024, 1024)\n",
    "                \n",
    "        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n",
    "        self.enable_mixup = False\n",
    "\n",
    "        # Define yourself dataset path\n",
    "        self.data_dir = \"datasets/COCO\"\n",
    "        self.train_ann = \"instances_train1024.json\"\n",
    "        self.val_ann = \"instances_valid1024.json\"\n",
    "\n",
    "        self.warmup_epochs = 3\n",
    "        self.num_classes = 1\n",
    "        self.max_epoch = 54 # 300\n",
    "        self.seed = 42\n",
    "        \n",
    "\n",
    "    def get_model(self, sublinear=False):\n",
    "\n",
    "        def init_yolo(M):\n",
    "            for m in M.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eps = 1e-3\n",
    "                    m.momentum = 0.03\n",
    "        if \"model\" not in self.__dict__:\n",
    "            from yolox.models import YOLOX, YOLOPAFPN, YOLOXHead\n",
    "            in_channels = [256, 512, 1024]\n",
    "            # NANO model use depthwise = True, which is main difference.\n",
    "            backbone = YOLOPAFPN(self.depth, self.width, in_channels=in_channels, depthwise=True)\n",
    "            head = YOLOXHead(self.num_classes, self.width, in_channels=in_channels, depthwise=True)\n",
    "            self.model = YOLOX(backbone, head)\n",
    "\n",
    "        self.model.apply(init_yolo)\n",
    "        self.model.head.initialize_biases(1e-2)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.cuda import amp\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/tmp/pip/cache/YOLOX/\")\n",
    "sys.path.append(\"../output/rsna-yolox/YOLOX/\")\n",
    "\n",
    "from yolox.utils import get_model_info, postprocess\n",
    "from yolox_utils import PadIfNeeded, scale_boxes, preprocess\n",
    "\n",
    "\n",
    "def init_roi(device_roi_='cuda:0'):\n",
    "    # To avoid name '__file__' is not defined\n",
    "    exp_file = \"/tmp/pip/cache/YOLOX/exps/example/custom/nano.py\"\n",
    "    sys.path.append(os.path.dirname(exp_file))\n",
    "    current_exp = importlib.import_module(os.path.basename(exp_file).split(\".\")[0])\n",
    "    exp = current_exp.Exp()\n",
    "    exp.test_conf = 0.0\n",
    "    exp.test_size = (1024, 1024)\n",
    "    exp.nmsthre = 0.45  # 0.65\n",
    "    model_roi_ = exp.get_model()\n",
    "    print(\"Model Summary: {}\".format(get_model_info(model_roi_, exp.test_size)))\n",
    "    model_roi_ = model_roi_.half().eval().to(device_roi_)\n",
    "    ckpt_file = (\n",
    "        \"../output/rsna-yolox/nano/best_ckpt_v2.pth\"  # mAP@(0.50:0.95)=0.9625\n",
    "    )\n",
    "    print(\"Loading YoloX weights\", ckpt_file)\n",
    "    ckpt = torch.load(ckpt_file, map_location=device_roi_)\n",
    "    model_roi_.load_state_dict(ckpt[\"model\"])\n",
    "    model_roi_.max_det = 1\n",
    "    model_roi_.nmsthre = exp.nmsthre\n",
    "    model_roi_.test_conf = exp.test_conf\n",
    "    model_roi_.test_size = exp.test_size\n",
    "    model_roi_.num_classes = 1\n",
    "    model_roi_.stride = 64\n",
    "    model_roi_.amp = True  # FP16\n",
    "    return model_roi_\n",
    "\n",
    "\n",
    "device_roi = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 'cpu'\n",
    "model_roi = None\n",
    "\n",
    "\n",
    "class Config:\n",
    "    IMAGE_BOX_SIZE_H = 1024\n",
    "    IMAGE_BOX_SIZE_W = 1024\n",
    "\n",
    "    image_pin_tr = PadIfNeeded(\n",
    "        min_height=IMAGE_BOX_SIZE_H, min_width=IMAGE_BOX_SIZE_W\n",
    "    )  # Pad to 1\n",
    "\n",
    "    YOLO_PREPROCESS_GPU = True\n",
    "    YOLO_SIZE = 1024\n",
    "\n",
    "\n",
    "def gpu_roi_crop(img, model_roi, config):\n",
    "    if model_roi is None:\n",
    "        raise \"Missing ROI model\"\n",
    "    # dt = (Profile(),Profile(),Profile())\n",
    "    # h, w = img.size(0), img.size(1)\n",
    "    # row_pd = pd.DataFrame([(h, w, -1, f)], columns=[\"h\", \"w\", \"confidence\", \"dcm_path\"])\n",
    "    row_pd = None\n",
    "\n",
    "    if config.YOLO_PREPROCESS_GPU is False:\n",
    "        # CPU, GPU round trip\n",
    "        results = model_roi(\n",
    "            (img * 255).squeeze(2).cpu().numpy().astype(np.uint8), size=config.YOLO_SIZE\n",
    "        )\n",
    "        row_pd = results.pandas().xyxy[0]\n",
    "    else:\n",
    "        # Full GPU\n",
    "        ims = [img.squeeze(2)]\n",
    "        imgs, files, shape0, shape1 = preprocess(\n",
    "            ims, size=config.YOLO_SIZE, model_stride=model_roi.stride\n",
    "        )  # Resize/Pad\n",
    "        img_roi = imgs[0]\n",
    "        # YoloX preprocessing: Pad to 1024x1024 (bgcolor=114., 0-255.)\n",
    "        # print(\"ROI img\", img_roi.shape, img_roi.max())\n",
    "        ratio = min(\n",
    "            model_roi.test_size[0] / img_roi.shape[0],\n",
    "            model_roi.test_size[1] / img_roi.shape[1],\n",
    "        )\n",
    "        padding = config.image_pin_tr(img_roi.shape[0], img_roi.shape[1])\n",
    "        img_roi = F.pad(\n",
    "            input=img_roi * 255.0,\n",
    "            pad=(\n",
    "                padding.get(\"pad_left\"),\n",
    "                padding.get(\"pad_right\"),\n",
    "                padding.get(\"pad_top\"),\n",
    "                padding.get(\"pad_bottom\"),\n",
    "            ),\n",
    "            mode=\"constant\",\n",
    "            value=114.0,\n",
    "        )\n",
    "        img_roi = (\n",
    "            img_roi.expand((3, img_roi.shape[0], img_roi.shape[1])).unsqueeze(0).float()\n",
    "        )  # (BCHW) float32\n",
    "        # print(\"YoloX re-padding completed\", img_roi.shape, img_roi.max())\n",
    "        with amp.autocast(model_roi.amp and (img_roi.device.type != \"cpu\")):\n",
    "            with torch.no_grad():\n",
    "                # y = model_roi(img_roi, size=YOLO_SIZE).cpu()\n",
    "                y = model_roi(img_roi)\n",
    "                y = postprocess(\n",
    "                    y,\n",
    "                    model_roi.num_classes,\n",
    "                    model_roi.test_conf,\n",
    "                    model_roi.nmsthre,\n",
    "                    class_agnostic=True,\n",
    "                )[0]\n",
    "                y = y.cpu().numpy()\n",
    "                bboxes = y[:, 0:4]\n",
    "                bboxes /= ratio\n",
    "                scores = y[:, 4] * y[:, 5]\n",
    "                row_pd = pd.DataFrame(bboxes, columns=[\"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "                row_pd[\"score\"] = scores\n",
    "                # Keep best bbox\n",
    "                row_pd = (\n",
    "                    row_pd.sort_values([\"score\"], ascending=False)\n",
    "                    .reset_index(drop=True)\n",
    "                    .iloc[0: model_roi.max_det, :]\n",
    "                )\n",
    "\n",
    "        # Unpad, scale back and clip if needed\n",
    "        row_pd[\"xmin\"] = row_pd[\"xmin\"] - padding.get(\"pad_left\")\n",
    "        row_pd[\"xmax\"] = row_pd[\"xmax\"] - padding.get(\"pad_right\")\n",
    "        row_pd[\"ymin\"] = row_pd[\"ymin\"] - padding.get(\"pad_top\")\n",
    "        row_pd[\"ymax\"] = row_pd[\"ymax\"] - padding.get(\"pad_bottom\")\n",
    "        y = np.expand_dims(row_pd[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values, axis=0)\n",
    "        scale_boxes(shape1, y[0][:, :4], shape0[0])\n",
    "        row_pd = pd.DataFrame(y[0], columns=[\"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "\n",
    "    if len(row_pd) == 1:\n",
    "        x1, x2 = int(row_pd[\"xmin\"]), int(row_pd[\"xmax\"])\n",
    "        y1, y2 = int(row_pd[\"ymin\"]), int(row_pd[\"ymax\"])\n",
    "        # Crop image\n",
    "        img = img[y1:y2, x1:x2]\n",
    "\n",
    "    return img, (y1, y2, x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_train[\"image_name\"] = df_train[[\"patient_id\", \"image_id\"]].apply(\n",
    "    lambda x : str(x[0]) + \"/\" + str(x[1]) + \".dcm\", axis=1\n",
    ")\n",
    "\n",
    "DATA_PATH = \"/workspace/\"\n",
    "PATH_TO_DCMS = DATA_PATH + \"data_rsna/train_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"/kaggle/input/rsna-breast-cancer-detection/train_images/\"\n",
    "test_images = glob.glob(f\"{IMG_PATH}*/*.dcm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIZE = (1024 + 512, 1024)\n",
    "# SIZE_SUFFIX = \"_\".join(map(str, list(SIZE))) if SIZE is not None else \"\"\n",
    "\n",
    "# SAVE_FOLDER = f\"../input/yolox_{SIZE_SUFFIX}/\"\n",
    "\n",
    "# files = df_train.image_name\n",
    "\n",
    "# print(f'Saving images to {SAVE_FOLDER}')\n",
    "# os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = ((1024) * 3, (1024) * 2)\n",
    "SIZE_SUFFIX = \"_\".join(map(str, list(SIZE))) if SIZE is not None else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FOLDER = f\"../input/yolox_{SIZE_SUFFIX}/\"\n",
    "\n",
    "files = df_train.image_name\n",
    "\n",
    "print(f'Saving images to {SAVE_FOLDER}')\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('../input/vindr-mammo/*.dicom')\n",
    "# PATH_TO_DCMS = \"\"\n",
    "\n",
    "# SAVE_FOLDER = f\"../input/vindr_yolox_{SIZE_SUFFIX}/\"\n",
    "\n",
    "# print(f'Saving images to {SAVE_FOLDER}')\n",
    "# os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('../input/cbis-ddcm/*.dcm')\n",
    "# PATH_TO_DCMS = \"\"\n",
    "\n",
    "# SAVE_FOLDER = f\"../input/cbis_yolox_{SIZE_SUFFIX}/\"\n",
    "\n",
    "# print(f'Saving images to {SAVE_FOLDER}')\n",
    "# os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('../input/CMMD/*/*/*/*.dcm')\n",
    "# PATH_TO_DCMS = \"\"\n",
    "\n",
    "# SAVE_FOLDER = f\"../input/cmmd_yolox_{SIZE_SUFFIX}/\"\n",
    "\n",
    "# print(f'Saving images to {SAVE_FOLDER}')\n",
    "# os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('../input/pasm/*/*/*.dcm')) + sorted(glob.glob('../input/pasm/*/*/*.DCM'))\n",
    "PATH_TO_DCMS = \"\"\n",
    "\n",
    "SAVE_FOLDER = f\"../input/pasm_yolox_{SIZE_SUFFIX}/\"\n",
    "\n",
    "print(f'Saving images to {SAVE_FOLDER}')\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_roi = init_roi(\"cuda:0\")\n",
    "\n",
    "for i, file in enumerate(tqdm(files)):    \n",
    "    if \"cbis\" in SAVE_FOLDER:\n",
    "        name = file.split(\"/\")[-1][:-4]\n",
    "    elif \"cmmd\" in SAVE_FOLDER:\n",
    "        dcm = pydicom.read_file(PATH_TO_DCMS + file, stop_before_pixels=True)\n",
    "        image = file.split(\"/\")[-1][:-4]\n",
    "        patient = file.split(\"/\")[-4]\n",
    "        name = f\"{patient}_{dcm.ImageLaterality}_{image}\"\n",
    "    elif \"pasm\" in SAVE_FOLDER:\n",
    "#         dcm = pydicom.read_file(PATH_TO_DCMS + file, stop_before_pixels=True)\n",
    "        image = file.split(\"/\")[-1][:-4]\n",
    "        patient = file.split(\"/\")[-3] + \"_\" + file.split(\"/\")[-2]\n",
    "        name = f\"{patient}_{image}\"\n",
    "    else:\n",
    "        patient = file.split(\"/\")[-2]\n",
    "        image = file.split(\"/\")[-1][:-4]\n",
    "        name = f\"{patient}_{image}\"\n",
    "    \n",
    "    if os.path.exists(SAVE_FOLDER + f\"{name}.png\"):\n",
    "        continue\n",
    "    img_main = read_normalize_basic(PATH_TO_DCMS + file)\n",
    "\n",
    "    try:\n",
    "        img_main, (y1, y2, x1, x2) = gpu_roi_crop(img_main.unsqueeze(2), model_roi, Config)\n",
    "    except:\n",
    "        print(f'Error ! with image {name}')\n",
    "\n",
    "    if SIZE is not None:\n",
    "        img_main = F.interpolate(\n",
    "            img_main.view(1, 1, img_main.size(0), img_main.size(1)),\n",
    "            SIZE,\n",
    "            mode=\"bilinear\"\n",
    "        )[0, 0]\n",
    "        \n",
    "    cv2.imwrite(SAVE_FOLDER + f\"{name}.png\", (img_main.cpu().squeeze().numpy() * 255).astype(np.uint8))\n",
    "\n",
    "    if i % 2000 == 0:\n",
    "        plt.imshow(img_main.cpu().squeeze().numpy(), cmap=\"gray\")\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
