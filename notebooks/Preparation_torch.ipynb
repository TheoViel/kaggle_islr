{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from data.preparation import prepare_data\n",
    "from utils.plots import plot_sample, plot_sample_with_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEPT_LANDMARKS = [\n",
    "    [468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488],  # left hand\n",
    "    [522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542],  # right hand\n",
    "    [10, 54, 67, 132, 150, 152, 162, 172, 176, 234, 284, 297, 361, 379, 389, 397, 400, 454],  # silhouette\n",
    "    [13, 37, 40, 61, 78, 81, 84, 87, 88, 91, 191, 267, 270, 291, 308, 311, 314, 317, 318, 321, 415],  # lips\n",
    "    [500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511], # arms\n",
    "    [205, 425],  # cheeks\n",
    "]\n",
    "MAPPING = [i + 1 for i in range(len(KEPT_LANDMARKS))]\n",
    "\n",
    "TO_AVG = [\n",
    "    [466, 387, 385, 398, 263, 390, 374, 381, 362],  # left_eye\n",
    "    [246, 160, 158, 173, 33, 163, 145, 154, 133],\n",
    "    [383, 293, 296, 285],  # left_eyebrow\n",
    "    [156, 63, 66, 55],  # right_eyebrow\n",
    "    [1, 2, 98, 327, 168],  # nose\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.concatenate(KEPT_LANDMARKS)\n",
    "type_embed = np.zeros(1000)\n",
    "start = 0\n",
    "for subset, idx in zip(KEPT_LANDMARKS, MAPPING):\n",
    "    print(subset, idx)\n",
    "    type_embed[start: start + len(subset)] = idx\n",
    "    start += len(subset)\n",
    "\n",
    "type_embed = type_embed[type_embed > 0]\n",
    "\n",
    "type_embed = np.concatenate([type_embed, np.array([idx] * len(TO_AVG))])\n",
    "\n",
    "print(\"\\nn_landmarks :\", len(type_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    \"\"\"\n",
    "    Loads a relevant subset of data from a Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        pq_path (str): Path to the Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, np.ndarray]: A tuple containing the DataFrame and the data array.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(pq_path)\n",
    "    n_frames = int(len(df) / ROWS_PER_FRAME)\n",
    "    data = df[['x', 'y', 'z']].values.reshape(n_frames, ROWS_PER_FRAME, 3)\n",
    "    return df, data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Preprocessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Preprocessing module for data preparation.\n",
    "    Module is compatible with onnx conversion, hence some weird operations.\n",
    "    \n",
    "    Attributes:\n",
    "        type_embed (torch.Tensor): Type embeddings.\n",
    "        landmark_embed (torch.Tensor): Landmark embeddings.\n",
    "        ids (torch.Tensor): Landmark IDs.\n",
    "        to_avg (List[torch.Tensor]): IDs of landmarks to average.\n",
    "        hands (torch.Tensor): IDs of hand landmarks.\n",
    "        frames (torch.Tensor): Frame numbers.\n",
    "        max_len (torch.Tensor): Maximum length.\n",
    "        \n",
    "    Methods:\n",
    "        filter_sign(self, x, truncate=False): Filters sign data by removing frames with missing hand landmarks,\n",
    "            and striding the sequence to a small enough dimension.\n",
    "        forward(self, x): Preprocesses the input sign data.\n",
    "    \"\"\"\n",
    "    def __init__(self, type_embed, max_len=50):\n",
    "        \"\"\"\n",
    "        Preprocessing module for data preparation.\n",
    "\n",
    "        Args:\n",
    "            type_embed (numpy array): Type embeddings.\n",
    "            max_len (int, optional): Maximum length. Defaults to 50.\n",
    "        \"\"\"\n",
    "        super(Preprocessing, self).__init__()\n",
    "\n",
    "        self.type_embed = torch.from_numpy(type_embed[None, :].astype(np.float32))\n",
    "        self.type_embed = self.type_embed.repeat(1000, 1)\n",
    "\n",
    "        self.landmark_embed = torch.tensor(np.arange(120)).float().unsqueeze(0) + 1\n",
    "        self.landmark_embed = self.landmark_embed.repeat(1000, 1)\n",
    "        \n",
    "        self.ids = torch.from_numpy(np.concatenate(KEPT_LANDMARKS))\n",
    "        self.to_avg = [torch.tensor(avg) for avg in TO_AVG]\n",
    "\n",
    "        self.hands = torch.tensor(\n",
    "            [468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488] + \n",
    "            [522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542]\n",
    "        )\n",
    "        self.frames = torch.tensor(np.arange(1000) + 1)\n",
    "        self.max_len = torch.tensor([max_len])\n",
    "\n",
    "    def filter_sign(self, x):\n",
    "        \"\"\"\n",
    "        Filters sign data by removing frames with missing hand landmarks,\n",
    "        and striding the sequence to a small enough dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Sign data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Filtered sign data.\n",
    "        \"\"\"\n",
    "        hands = x[:, self.hands, 0]\n",
    "        nan_prop = torch.isnan(hands).float().mean(-1)\n",
    "        x = x[torch.where(nan_prop < 1)[0]]\n",
    "\n",
    "        length = self.frames[:x.size(0)].max().unsqueeze(0)\n",
    "        sz = torch.cat([length, self.max_len]).max()\n",
    "        \n",
    "        divisor = (((sz - self.max_len) > 0) * (sz / self.max_len) + 1).int()\n",
    "        ids = (self.frames[:x.size(0)] % divisor) == 0\n",
    "        \n",
    "        return x[ids]\n",
    "\n",
    "    def forward(self, x): \n",
    "        \"\"\"\n",
    "        Forward pass of the preprocessing module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sign data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed sign data.\n",
    "        \"\"\"\n",
    "        x = self.filter_sign(x)\n",
    "            \n",
    "        n_frames = x.shape[0]     \n",
    "        \n",
    "        avg_ids = []\n",
    "        for ids in self.to_avg:\n",
    "            avg_id = x[:, ids].mean(1, keepdims=True)\n",
    "            avg_ids.append(avg_id)\n",
    "\n",
    "        x = torch.cat([x[:, self.ids]] + avg_ids, 1)\n",
    "\n",
    "        type_embed = self.type_embed[:n_frames]\n",
    "        landmark_embed = self.landmark_embed[:n_frames, :x.shape[1]]\n",
    "        \n",
    "        # Normalize & fill nans\n",
    "        nonan = x[~torch.isnan(x)].view(-1, x.shape[-1])\n",
    "        x = x - nonan.mean(0)[None, None, :]\n",
    "        x = x / nonan.std(0, unbiased=False)[None, None, :]\n",
    "        x[torch.isnan(x)] = 0\n",
    "\n",
    "        # Concat\n",
    "        x = torch.cat([\n",
    "            type_embed.unsqueeze(-1).to(x.device), x # , landmark_embed.unsqueeze(-1)\n",
    "        ], -1).transpose(1, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(DATA_PATH, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = Preprocessing(type_embed, max_len=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "\n",
    "SAVE_FOLDER = \"../input/torch_12/\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(df['path']))):\n",
    "    path = df['path'][i]\n",
    "    name = f\"{path.split('/')[-2]}_{path.split('/')[-1].split('.')[0]}.npy\"\n",
    "    \n",
    "    if SAVE and os.path.exists(SAVE_FOLDER + name):\n",
    "        continue\n",
    "\n",
    "    pq, data = load_relevant_data_subset(path)\n",
    "\n",
    "    data = torch.from_numpy(data).cuda()\n",
    "    \n",
    "    out_torch = prepro(data).cpu()\n",
    "\n",
    "    if SAVE:\n",
    "        np.save(SAVE_FOLDER + name, out_torch.numpy())\n",
    "\n",
    "    if not (i % 30000):\n",
    "        data = {\n",
    "            \"x\": out_torch[:, 1],\n",
    "            \"y\": out_torch[:, 2],\n",
    "            \"z\": out_torch[:, 3],\n",
    "            \"type\": out_torch[:, 0],\n",
    "        }\n",
    "\n",
    "        plot_sample_with_edges(data, n_frames=4, figsize=(10, 10))\n",
    "\n",
    "    if not SAVE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
