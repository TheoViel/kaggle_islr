{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to infer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logger import Config, upload_to_kaggle\n",
    "\n",
    "from params import *\n",
    "from data.dataset import SignDataset\n",
    "from data.preparation import *\n",
    "\n",
    "from utils.metrics import *\n",
    "from utils.plots import plot_sample\n",
    "from utils.plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_FOLDER = \"../logs/2023-03-16/42/\"\n",
    "EXP_FOLDER = \"../logs/2023-03-19/0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(DATA_PATH, config.processed_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"fold\" not in df.columns:\n",
    "    folds = pd.read_csv(config.folds_file)\n",
    "    df = df.merge(folds, how=\"left\", on=[\"participant_id\", \"sequence_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_oof = np.load(EXP_FOLDER + \"pred_oof.npy\")\n",
    "score = accuracy(df['target'], pred_oof)\n",
    "print(f\"-> CV acc : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = pred_oof.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error'] = (df['target'] != df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('participant_id').agg(['mean', 'count', 'sum'])[['error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.sort_values(('error',  'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('sign').agg('mean')[['error']].sort_values('error', ascending=False).T\n",
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = json.load(open(DATA_PATH + \"sign_to_prediction_index_map.json\", \"r\"))\n",
    "classes = list(classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df['target'], df['pred'], normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    y_pred,\n",
    "    y_true,\n",
    "    cm=None,\n",
    "    normalize=\"true\",\n",
    "    display_labels=None,\n",
    "    cmap=\"viridis\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes and plots a confusion matrix.\n",
    "    Args:\n",
    "        y_pred (numpy array): Predictions.\n",
    "        y_true (numpy array): Truths.\n",
    "        normalize (bool or None, optional): Whether to normalize the matrix. Defaults to None.\n",
    "        display_labels (list of strings or None, optional): Axis labels. Defaults to None.\n",
    "        cmap (str, optional): Colormap name. Defaults to \"viridis\".\n",
    "    \"\"\"\n",
    "    if cm is None:\n",
    "        cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "#     cm = cm[::-1, :]\n",
    "\n",
    "    # Display colormap\n",
    "    n_classes = cm.shape[0]\n",
    "    im_ = plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "\n",
    "    # Display values\n",
    "    cmap_min, cmap_max = im_.cmap(0), im_.cmap(256)\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "    for i in tqdm(range(n_classes)):\n",
    "        for j in range(n_classes):\n",
    "            if cm[i, j] > 0.1:\n",
    "                color = cmap_max if cm[i, j] < thresh else cmap_min\n",
    "                text = f\"{cm[i, j]:.0f}\" if normalize is None else f\"{cm[i, j]:.1f}\"\n",
    "                plt.text(j, i, text, ha=\"center\", va=\"center\", color=color)\n",
    "\n",
    "    # Display legend\n",
    "    plt.xlim(-0.5, n_classes - 0.5)\n",
    "    plt.ylim(-0.5, n_classes - 0.5)\n",
    "    if display_labels is not None:\n",
    "        plt.xticks(np.arange(n_classes), display_labels)\n",
    "        plt.yticks(np.arange(n_classes), display_labels)\n",
    "\n",
    "    plt.ylabel(\"True label\", fontsize=12)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "plot_confusion_matrix(df['pred'], df['target'], display_labels=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
