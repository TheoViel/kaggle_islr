{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to validate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.preparation import *\n",
    "from data.dataset import SignDataset\n",
    "\n",
    "from utils.metrics import *\n",
    "from utils.plots import *\n",
    "from utils.logger import Config\n",
    "\n",
    "from inference.main import kfold_inference_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(DATA_PATH, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_FOLDER = \"../logs/2023-04-17/42/\"  # 0.7265 / x5 0.7273 / MTx10 0.7274\n",
    "# EXP_FOLDER = \"../logs/2023-04-18/10/\"  # 0.7245 / x5 0.7247 / MTx10 0.7249\n",
    "# EXP_FOLDER = \"../logs/2023-04-19/6/\"  # 0.7245 / x10 0.7254 / MTx10 0.7254\n",
    "# EXP_FOLDER = \"../logs/2023-04-20/18/\"  # 0.7247 / x10 0.7254 / MTx10 0.7256\n",
    "# EXP_FOLDER = \"../logs/2023-04-20/19/\"  # 0.7250 / x10 0.7257 / MTx10 0.7254\n",
    "# EXP_FOLDER = \"../logs/2023-04-20/21/\"  # 0.7243 / x10 0.7246 / MTx10 0.7244\n",
    "\n",
    "FILES = [  #  CV 0.7279 - LB 0.78\n",
    "    \"../logs/2023-04-14/37/pred_oof_dist.npy\",  # 0.7225 / Dist 0.7196 (-0.003)\n",
    "    \"../logs/2023-04-15/7/pred_oof_dist.npy\",  # 0.7228 / Dist 0.7176 (-0.005)\n",
    "]\n",
    "\n",
    "FILES = [  # CV 0.7340 - LB 0.78\n",
    "    \"../logs/2023-04-20/19/pred_oof_inf.npy\",  # 0.7257\n",
    "    \"../logs/2023-04-20/18/pred_oof_mt.npy\",  # 0.7254\n",
    "]\n",
    "\n",
    "\n",
    "# FILES = [  # CV 0.7351\n",
    "# #     \"../logs/2023-04-22/6/pred_oof_mt.npy\",  # 0.7298\n",
    "#     \"../logs/2023-04-22/7/pred_oof_mt.npy\",  # 0.7310\n",
    "# #     \"../logs/2023-04-22/2/pred_oof.npy\",  # 0.7310\n",
    "# ]\n",
    "\n",
    "# WEIGHTS = [1, 1]\n",
    "\n",
    "try:\n",
    "    pred_oof = np.average([np.load(f) for f in FILES], weights=WEIGHTS, axis=0)\n",
    "    print('- Weighted average')\n",
    "except:\n",
    "    print('- Simple average')\n",
    "    pred_oof = np.mean([np.load(f) for f in FILES], 0)\n",
    "df['pred'] = pred_oof.argmax(-1)\n",
    "\n",
    "score = accuracy(df['target'], pred_oof)\n",
    "print(f\"\\n-> CV acc : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = \"../logs/2023-04-21/31/\"  # 0.7267\n",
    "# EXP_FOLDER = \"../logs/2023-04-22/2/\"  # 0.7310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(DATA_PATH, config.processed_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"fold\" not in df.columns:\n",
    "    folds = pd.read_csv(config.folds_file)\n",
    "    df = df.merge(folds, how=\"left\", on=[\"participant_id\", \"sequence_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_oof = np.load(EXP_FOLDER + \"pred_oof.npy\")\n",
    "df['pred'] = pred_oof.argmax(-1)\n",
    "\n",
    "score = accuracy(df['target'], pred_oof)\n",
    "print(f\"-> CV acc : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['participant_id'] != 29302].reset_index(drop=True)\n",
    "# score = accuracy(df['target'], df['pred'])\n",
    "# print(f\"-> CV acc : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error'] = (df['target'] != df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('participant_id').agg(['mean', 'count', 'sum'])[['error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.sort_values(('error',  'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('sign').agg('mean')[['error']].sort_values('error', ascending=False).T\n",
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = json.load(open(DATA_PATH + \"sign_to_prediction_index_map.json\", \"r\"))\n",
    "classes = list(classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df['target'], df['pred'], normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(classes))):  # gt\n",
    "    for j in range(len(classes)):\n",
    "        n = cm[i, j]\n",
    "        if n > 50 and i != j:\n",
    "            s = f\"{classes[i]} predicted as {classes[j]} :\".ljust(32)\n",
    "            print(f\"{s} {n} / {cm[i].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(50, 50))\n",
    "# plot_confusion_matrix(df['pred'], df['target'], display_labels=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inf val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = \"../logs/2023-04-17/42/\"  # 0.7265 / x5 0.7273 / MTx10 0.7274\n",
    "EXP_FOLDER = \"../logs/2023-04-18/10/\"  # 0.7245 / x5 0.7247 / MTx10 0.7249\n",
    "# EXP_FOLDER = \"../logs/2023-04-19/6/\"  # 0.7245 / x10 0.7254 / MTx10 0.7254\n",
    "# EXP_FOLDER = \"../logs/2023-04-20/18/\"  # 0.7247 / x10 0.7254 / MTx10 0.7256\n",
    "# EXP_FOLDER = \"../logs/2023-04-20/19/\"  # 0.7250 / x10 0.7257 / MTx10 0.7254\n",
    "# EXP_FOLDER = \"../logs/2023-04-20/21/\"  # 0.7243 / x10 0.7246 / MTx10 0.7244\n",
    "\n",
    "EXP_FOLDER = \"../logs/2023-04-21/9/\"  # 0.7262 / x10 0.7269 / MTx10 0.7268\n",
    "\n",
    "EXP_FOLDER = \"../logs/2023-04-21/31/\"  # 0.7267 / x10 0.7278 / MTx10 0.7278\n",
    "\n",
    "EXP_FOLDER = \"../logs/2023-04-22/6/\"   # 0.7294 / x10 0.7296 / MTx10 0.7298\n",
    "# EXP_FOLDER = \"../logs/2023-04-22/7/\"   # 0.7295 / x10 0.7303 / MTx10 0.7310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))\n",
    "df = prepare_data(DATA_PATH, config.processed_folder)\n",
    "\n",
    "if \"fold\" not in df.columns:\n",
    "    folds = pd.read_csv(config.folds_file)\n",
    "    df = df.merge(folds, how=\"left\", on=[\"participant_id\", \"sequence_id\"])\n",
    "    \n",
    "try:\n",
    "    pred_oof = np.load(EXP_FOLDER + \"pred_oof.npy\")\n",
    "    df['pred'] = pred_oof.argmax(-1)\n",
    "\n",
    "    score = accuracy(df['target'], pred_oof)\n",
    "    print(f\"-> CV acc : {score:.4f}\")\n",
    "except:\n",
    "    df = df[df['fold'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    pred_oof = np.load(EXP_FOLDER + \"pred_val_0.npy\")\n",
    "    df['pred'] = pred_oof.argmax(-1)\n",
    "\n",
    "    score = accuracy(df['target'], pred_oof)\n",
    "    print(f\"-> Fold 0 acc : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_oof = kfold_inference_val(\n",
    "    df,\n",
    "    EXP_FOLDER,\n",
    "    debug=False,\n",
    "    save=True,\n",
    "    use_fp16=True,\n",
    "    use_mt=False,\n",
    "    distilled=False,\n",
    "    n_soup=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = (pred_oof + np.load(EXP_FOLDER + \"pred_val_0.npy\")) / 2\n",
    "# accuracy(df['target'], pred)\n",
    "\n",
    "# df_val['pred_flip'] = pred.argmax(-1)\n",
    "# df_val['error_flip'] = (df_val['pred_flip'] != df_val['target'])\n",
    "\n",
    "# dfg = df_val.groupby('sign')[[\"error\", \"error_flip\"]].mean()\n",
    "# dfg[\"delta\"] = dfg[\"error\"] - dfg[\"error_flip\"]\n",
    "\n",
    "# dfg.sort_values('delta').T\n",
    "\n",
    "# [classes.index(c) for c in dfg[dfg['delta'] < 0.01].index]\n",
    "\n",
    "# [classes.index(c) for c in dfg[dfg['delta'] < -0.05].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inf train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = \"../logs/2023-04-11/27/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))\n",
    "df = prepare_data(DATA_PATH, config.processed_folder)\n",
    "\n",
    "if \"fold\" not in df.columns:\n",
    "    folds = pd.read_csv(config.folds_file)\n",
    "    df = df.merge(folds, how=\"left\", on=[\"participant_id\", \"sequence_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_oof = kfold_inference_val(\n",
    "    df,\n",
    "    EXP_FOLDER,\n",
    "    debug=False,\n",
    "    save=False,\n",
    "    use_fp16=True,\n",
    "    train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(EXP_FOLDER + \"pred_oof_train.npy\", pred_oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_0'] = pred_oof[0].argmax(-1)\n",
    "df_val = df[df['fold'] != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = df_val[(df_val['target'] != df_val['pred_0'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SignDataset(df_err, max_len=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(dataset))):\n",
    "#     i = 92284\n",
    "    data = dataset[i]\n",
    "    \n",
    "#     for k in data.keys():\n",
    "#         print(k, data[k].size())\n",
    "    \n",
    "    print(df_err['sequence_id'][i], \"- pred :\", classes[df_err['pred_0'][i]], \" - truth :\", df_err['sign'][i])\n",
    "    plot_sample_with_edges(data, n_frames=4, figsize=(10, 10), show_text=False)\n",
    "\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
